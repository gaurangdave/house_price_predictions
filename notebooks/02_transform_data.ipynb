{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transform Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* In this notebook we'll create custom transformers and pipelines to transform the raw `training` data into transformed data for ML training.\n",
    "* We'll use the same pipeline to transform the data for prediction as well. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "## import the necessary libraries\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "from sklearn.model_selection import train_test_split, StratifiedShuffleSplit\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder, MinMaxScaler, StandardScaler, FunctionTransformer\n",
    "from sklearn.metrics.pairwise import rbf_kernel\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from scipy.signal import find_peaks\n",
    "from scipy.stats import gaussian_kde\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.utils.validation import check_array, check_is_fitted\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.cluster import KMeans\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_data_path = Path(\"..\", \"data\", \"processed\", \"housing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "## read data\n",
    "data = pd.read_csv(Path(processed_data_path, \"train_set.csv\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split Features & Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "## before we create the pipeline lets split he training data into features and labels\n",
    "df_features = data.drop(\"median_house_value\", axis=1)\n",
    "df_labels = data[\"median_house_value\"].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Numeric Transformations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fill in missing values\n",
    "* Using `SimpleImputer` with `median` strategy to fill in all the missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_impute_pipeline = Pipeline([\n",
    "    (\"impute categories\", SimpleImputer(strategy=\"median\")),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "## uncomment the code below to test the pipeline\n",
    "# df = pd.DataFrame({'a': [1,np.nan,3], 'b': [10, 30, 40], 'c': [10, 10, 10]})\n",
    "# numeric_impute_pipeline.fit_transform(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Engineering "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ratios\n",
    "* `rooms_per_house` - `total_rooms` to `households` ratio\n",
    "* `bedroom_room_ratio` - `total_bedrooms` to `total_rooms` ration\n",
    "* `people_per_house` - `population` to `households` ratio\n",
    "* We'll create a simple `Function Transformer` that calculates the ratio between first two columns\n",
    "* `Hyperparameters` : N/A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_ratio_feature_names(function_transformer, feature_names_in):\n",
    "    \"\"\"Calculate the ratio feature names.\"\"\"\n",
    "    # feature_name = f\"{feature_names_in[0]}_to_{feature_names_in[1]}_ratio\"\n",
    "    return [\"ratio\"]  # feature names out\n",
    "\n",
    "def calculate_ratio(df):\n",
    "    \"\"\"Calculate the ratio of the first two columns.\"\"\"\n",
    "    # if not df.iloc[:, [0, 1]].map(np.isreal).all().all():\n",
    "    #     raise ValueError(\"Columns are not numbers\")\n",
    "    # feature_names = df.iloc[:, [0, 1]].columns\n",
    "    # col_name = calculate_ratio_feature_names(calculate_ratio, feature_names)\n",
    "    result = df[:, [0]] / df[:, [1]]\n",
    "    # result.columns = col_name\n",
    "    return result\n",
    "\n",
    "feature_ratio_transformer = FunctionTransformer(calculate_ratio, feature_names_out=calculate_ratio_feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## uncomment the code to quickly test to validate the transformer\n",
    "# df = pd.DataFrame({'a': [1, 2, 3], 'b': [10, 30, 40], 'c': [10, 10, 10]})\n",
    "# ## creating a column transfomer\n",
    "# test_transformer = ColumnTransformer([\n",
    "#     (\"test\", feature_ratio_transformer, [\"a\", \"b\"])\n",
    "# ])\n",
    "# pd.DataFrame(test_transformer.fit_transform(df), columns=test_transformer.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Handling Multimodal Data\n",
    "* `housing_median_age` is a multi modal distribution, so we'll calculate similarity to all the modes.\n",
    "* We can either use a static list of modes to calculate the similarity in that case we can use simple `Function Transformer`\n",
    "* We can use dynamic list of modes using gaussian KDE everytime, in this case we can use `Transformer Class` to `fit` and `transform`\n",
    "* `Hyperparameters` : `gamma`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultimodalTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, n_clusters=5, gamma=0.1):\n",
    "        self.n_clusters = n_clusters\n",
    "        self.gamma = gamma\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        checked_X = check_array(X)\n",
    "        self.X_ = checked_X\n",
    "        self.feature_data_ = self.X_[:, 0]\n",
    "        self.kde_ = gaussian_kde(self.feature_data_)\n",
    "        self.x_grid_ = np.linspace(self.feature_data_.min(), self.feature_data_.max(), 1000)\n",
    "        self.kde_values_ = self.kde_.evaluate(self.x_grid_)\n",
    "        self.peaks_, _ = find_peaks(self.kde_values_)\n",
    "        self.peaks_values_ = self.x_grid_[self.peaks_]\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        check_is_fitted(self, ['X_', 'feature_data_', 'kde_', 'x_grid_', 'kde_values_', 'peaks_'])\n",
    "        checked_X = check_array(X)\n",
    "        feature_data = checked_X[:, 0].reshape(-1, 1)\n",
    "        similarity_matrix = np.zeros((feature_data.shape[0], len(self.peaks_)))\n",
    "        for i, peak in enumerate(self.peaks_):\n",
    "            peak_value = self.x_grid_[peak]\n",
    "            similarity_matrix[:, i] = rbf_kernel(feature_data, [[peak_value]], gamma=self.gamma).flatten()\n",
    "        return similarity_matrix\n",
    "\n",
    "    def get_feature_names_out(self, input_features=None):\n",
    "        return [f\"similarity_to_peak_{round(peak)}\" for peak in self.peaks_values_]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "## uncomment the code below to test the transformer with mock data\n",
    "# lets test the transformer with mock data\n",
    "# df = pd.DataFrame({'a': [1, 2, 2, 2, 3,1 ,4,7,7,8,9,5, 5, 5,5,10, 10, 10,10, 10,10, 10,10, 10,10]})\n",
    "\n",
    "# cluster_similarity_transformer = MultimodalTransformer(\n",
    "#     n_clusters=5, gamma=0.1)\n",
    "# # cluster_similarity_transformer.fit_transform(df)\n",
    "\n",
    "# pd.DataFrame(cluster_similarity_transformer.fit_transform(df), columns=cluster_similarity_transformer.get_feature_names_out())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Location Cluster Similarity\n",
    "* Since we are dealing with location data, we can find similarity between different locations\n",
    "* One approach is to identify different cluster, and then calculate similarity between all the datapoints and that cluster. \n",
    "* We'll need to create a transformer for that.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClusterSimilarityTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, n_clusters=10, gamma=1.0, random_state=None):\n",
    "        self.n_clusters = n_clusters\n",
    "        self.gamma = gamma\n",
    "        self.random_state = random_state\n",
    "\n",
    "    def fit(self, X, y=None, sample_weight=None):\n",
    "        self.kmeans_ = KMeans(self.n_clusters, n_init=10,\n",
    "                              random_state=self.random_state)\n",
    "        self.kmeans_.fit(X, sample_weight=sample_weight)\n",
    "        return self  # always return self!\n",
    "\n",
    "    def transform(self, X):\n",
    "        check_is_fitted(self, ['kmeans_'])\n",
    "        return rbf_kernel(X, self.kmeans_.cluster_centers_, gamma=self.gamma)\n",
    "\n",
    "    def get_feature_names_out(self, input_features=None):\n",
    "        return [f\"similarity_to_cluster_{i}\" for i in range(self.n_clusters)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dropping Outliers\n",
    "* Custom Class Transformer since we'll need to fit the data using `IsolationForest`\n",
    "* `Hyperparameters` : `remove_outlier` a boolean hyper parameter to decide whether to drop outlier or not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OutlierRemover(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, remove_outliers=True):\n",
    "        self.remove_outliers = remove_outliers\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        check_array(X)\n",
    "        self.iforest_ = IsolationForest(random_state=42)\n",
    "        self.outlier_prediction_ = self.iforest_.fit_predict(X)\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        check_is_fitted(self, ['iforest_', 'outlier_prediction_'])\n",
    "        check_array(X)\n",
    "        return X.iloc[self.outlier_prediction_ == 1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "## uncomment the code below to test the transformer with mock data\n",
    "# lets test the transformer with mock data\n",
    "# df = pd.DataFrame({'a': [1, 2, 2, 2, 3,1 ,4,7,7,8,9,5, 5, 5,5,10, 10, 10,10, 10,10, 10,10, 10,10]})\n",
    "# outlier_remover = OutlierRemover()\n",
    "# outlier_remover.fit_transform(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* After thinking about this transformer, I realized that since it reduces the number of rows, it could cause problems in the pipelines's downstream transformers. \n",
    "* One options is to remove the outliers seperate from the pipeline, second one is to use a feature column to inform downstream transformers, but that might complicate code. \n",
    "* For now I'll handle the outliers seperate from the pipeline. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transform Heavy Tailed Features\n",
    "* Transform heavy tailed features using logarithm \n",
    "* Simple Function Transformer to find `np.log` and `np.exp` as inverse transformation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def heavy_tail_distribution(df):\n",
    "    \"\"\"Transform heavy tailed distribution using logarithm.\"\"\"\n",
    "    return np.log1p(df)\n",
    "\n",
    "heavy_tail_transformer = FunctionTransformer(heavy_tail_distribution, inverse_func=np.expm1, feature_names_out=\"one-to-one\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scaling\n",
    "* Scale all numeric features. \n",
    "* Custom Class Transformer since we need to fit and transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standard_scaler(df):\n",
    "    \"\"\"Scale features using StandardScaler.\"\"\"\n",
    "    return StandardScaler().fit_transform(df)\n",
    "\n",
    "standard_scaler_transformer = FunctionTransformer(standard_scaler)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Categorical Transformations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Convert `ocean_proximity` to one hot encoding\n",
    "* Custom Class Transformer cause we'll need to fit/trasnform using OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_pipeline = Pipeline([\n",
    "    (\"impute_categories\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "    (\"encode_categories\", OneHotEncoder(sparse_output=False))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0          NEAR BAY\n",
       "1         <1H OCEAN\n",
       "2            INLAND\n",
       "3            INLAND\n",
       "4        NEAR OCEAN\n",
       "            ...    \n",
       "16507     <1H OCEAN\n",
       "16508        INLAND\n",
       "16509    NEAR OCEAN\n",
       "16510     <1H OCEAN\n",
       "16511    NEAR OCEAN\n",
       "Name: ocean_proximity, Length: 16512, dtype: object"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_features[\"ocean_proximity\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 1., 0.],\n",
       "       [1., 0., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., 0., 1.],\n",
       "       [1., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 1.]])"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## testing to see if pipelines worked\n",
    "cat_pipeline.fit_transform(df_features.select_dtypes(include=object))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['ocean_proximity_<1H OCEAN', 'ocean_proximity_INLAND',\n",
       "       'ocean_proximity_ISLAND', 'ocean_proximity_NEAR BAY',\n",
       "       'ocean_proximity_NEAR OCEAN'], dtype=object)"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cat_pipeline.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ocean_proximity_&lt;1H OCEAN</th>\n",
       "      <th>ocean_proximity_INLAND</th>\n",
       "      <th>ocean_proximity_ISLAND</th>\n",
       "      <th>ocean_proximity_NEAR BAY</th>\n",
       "      <th>ocean_proximity_NEAR OCEAN</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16507</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16508</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16509</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16510</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16511</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>16512 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       ocean_proximity_<1H OCEAN  ocean_proximity_INLAND  \\\n",
       "0                            0.0                     0.0   \n",
       "1                            1.0                     0.0   \n",
       "2                            0.0                     1.0   \n",
       "3                            0.0                     1.0   \n",
       "4                            0.0                     0.0   \n",
       "...                          ...                     ...   \n",
       "16507                        1.0                     0.0   \n",
       "16508                        0.0                     1.0   \n",
       "16509                        0.0                     0.0   \n",
       "16510                        1.0                     0.0   \n",
       "16511                        0.0                     0.0   \n",
       "\n",
       "       ocean_proximity_ISLAND  ocean_proximity_NEAR BAY  \\\n",
       "0                         0.0                       1.0   \n",
       "1                         0.0                       0.0   \n",
       "2                         0.0                       0.0   \n",
       "3                         0.0                       0.0   \n",
       "4                         0.0                       0.0   \n",
       "...                       ...                       ...   \n",
       "16507                     0.0                       0.0   \n",
       "16508                     0.0                       0.0   \n",
       "16509                     0.0                       0.0   \n",
       "16510                     0.0                       0.0   \n",
       "16511                     0.0                       0.0   \n",
       "\n",
       "       ocean_proximity_NEAR OCEAN  \n",
       "0                             0.0  \n",
       "1                             0.0  \n",
       "2                             0.0  \n",
       "3                             0.0  \n",
       "4                             1.0  \n",
       "...                           ...  \n",
       "16507                         0.0  \n",
       "16508                         0.0  \n",
       "16509                         1.0  \n",
       "16510                         0.0  \n",
       "16511                         1.0  \n",
       "\n",
       "[16512 rows x 5 columns]"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(cat_pipeline.fit_transform(df_features.select_dtypes(include=object)), columns=cat_pipeline.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combining the Transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to following pipelines in a column transformer\n",
    "* Ratio Pipeline : Simple Imputer + Ratio Function Transformers + Scaling\n",
    "* Similarity Pipeline : Simple Imputer + Multimodal Similarity (Housing Median Age)\n",
    "* Similarity Pipeline : Simple Imputer + Cluster Similarity (Lat, Long)\n",
    "* Log Pipeline : Simple Imputer (Most Frequent) + Log + Scaling\n",
    "* Category Pipeline : Simple Imputer (most frequent) + Onehot Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratio_pipeline = Pipeline([\n",
    "    (\"impute_ratios\", SimpleImputer(strategy=\"median\")),\n",
    "    (\"calculate_ratios\", feature_ratio_transformer),\n",
    "    (\"standard_scaler\", StandardScaler())     \n",
    "])\n",
    "\n",
    "multimodal_similarity_pipeline = Pipeline([\n",
    "    (\"impute_multimodal\", SimpleImputer(strategy=\"median\")),\n",
    "    (\"calculate_multimodal\", MultimodalTransformer(n_clusters=5, gamma=0.1)),\n",
    "    (\"standard_scaler\", StandardScaler())\n",
    "])\n",
    "\n",
    "## most frequent imputer for latitude and longitude\n",
    "cluster_similarity_pipeline = Pipeline([\n",
    "    (\"impute_cluster\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "    (\"calculate_cluster\", ClusterSimilarityTransformer(n_clusters=10, gamma=1, random_state=42)),\n",
    "    (\"standard_scaler\", StandardScaler())\n",
    "])\n",
    "\n",
    "log_pipeline = Pipeline([\n",
    "    (\"impute_log\", SimpleImputer(strategy=\"median\")),\n",
    "    (\"log_transform\", heavy_tail_transformer),\n",
    "    (\"standard_scaler\", StandardScaler())\n",
    "])\n",
    "\n",
    "## copy/pasting the pipeline from above for easy access\n",
    "cat_pipeline = Pipeline([\n",
    "    (\"impute_categories\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "    (\"encode_categories\", OneHotEncoder(sparse_output=False))\n",
    "])\n",
    "\n",
    "## lets create a full pipeline\n",
    "preprocessing_pipeline = ColumnTransformer([\n",
    "    (\"bedrooms_per_room\", ratio_pipeline, [\"total_bedrooms\", \"total_rooms\"]),\n",
    "    (\"rooms_per_household\", ratio_pipeline, [\"total_rooms\", \"households\"]),\n",
    "    (\"population_per_household\", ratio_pipeline, [\"population\", \"households\"]),\n",
    "    (\"multimodal_similarity\", multimodal_similarity_pipeline, [\"housing_median_age\"]),\n",
    "    (\"cluster_similarity\", cluster_similarity_pipeline, [\"latitude\", \"longitude\"]),\n",
    "    (\"log_pipeline\", log_pipeline, [\"total_bedrooms\", \"total_rooms\", \"population\", \"households\", \"median_income\"]),\n",
    "    (\"categorical\", cat_pipeline, [\"ocean_proximity\"])\n",
    "])    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16512, 27)"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessed_data = preprocessing_pipeline.fit_transform(df_features)\n",
    "preprocessed_data.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['bedrooms_per_room__ratio', 'rooms_per_household__ratio',\n",
       "       'population_per_household__ratio',\n",
       "       'multimodal_similarity__similarity_to_peak_17',\n",
       "       'multimodal_similarity__similarity_to_peak_26',\n",
       "       'multimodal_similarity__similarity_to_peak_35',\n",
       "       'multimodal_similarity__similarity_to_peak_52',\n",
       "       'cluster_similarity__similarity_to_cluster_0',\n",
       "       'cluster_similarity__similarity_to_cluster_1',\n",
       "       'cluster_similarity__similarity_to_cluster_2',\n",
       "       'cluster_similarity__similarity_to_cluster_3',\n",
       "       'cluster_similarity__similarity_to_cluster_4',\n",
       "       'cluster_similarity__similarity_to_cluster_5',\n",
       "       'cluster_similarity__similarity_to_cluster_6',\n",
       "       'cluster_similarity__similarity_to_cluster_7',\n",
       "       'cluster_similarity__similarity_to_cluster_8',\n",
       "       'cluster_similarity__similarity_to_cluster_9',\n",
       "       'log_pipeline__total_bedrooms', 'log_pipeline__total_rooms',\n",
       "       'log_pipeline__population', 'log_pipeline__households',\n",
       "       'log_pipeline__median_income',\n",
       "       'categorical__ocean_proximity_<1H OCEAN',\n",
       "       'categorical__ocean_proximity_INLAND',\n",
       "       'categorical__ocean_proximity_ISLAND',\n",
       "       'categorical__ocean_proximity_NEAR BAY',\n",
       "       'categorical__ocean_proximity_NEAR OCEAN'], dtype=object)"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessing_pipeline.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lessons Learnt:\n",
    "* Transformers + Piplelines can be tricky specially with column names and expected datatypes. To build more resilient and production ready pipelines we'll need to add a lot of checks and validation.\n",
    "* Naming is equally important, the column names after preprocessing are not so ideal. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
